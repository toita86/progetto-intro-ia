{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto Introduzione all'intelligenza artificiale\n",
    "\n",
    "Il progetto consiste nella realizzazione di una applicazione di Intelligenza Artificiale completa degli\n",
    "aspetti di gestione di: sensori per l’acquisizione dei dati dall’esterno relativi a stati e obiettivi,\n",
    "ragionamento/ricerca della soluzione per i goal acquisiti, esecutori per la realizzazione delle azioni\n",
    "che conducono alla soluzione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progetto Uniform Coloring\n",
    "Uniform Coloring è un dominio in cui si hanno a disposizione alcune celle da colorare, e vari colori a disposizione. \n",
    "Per semplicità immaginiamo una griglia rettangolare in cui è possibile spostare una testina colorante fra le celle attigue secondo le 4 direzioni cardinali (N,S,E,W), senza uscire dalla griglia. \n",
    "\n",
    "Tutte le celle hanno un colore di partenza (B=blu, Y=yellow, G=green) ad eccezione di quella in cui si trova la testina indicata con T. La testina può colorare la cella in cui si trova con uno qualsiasi dei colori disponibili a differenti costi (cost(B)=1, cost(Y)=2, cost(G)=3), mentre gli spostamenti hanno tutti costo uniforme pari a 1. \n",
    "\n",
    ">L’obiettivo è colorare tutte le celle dello stesso colore (non importa quale) e riportare la testina nella sua posizione di partenza.\n",
    "\n",
    "La codifica di tutto il dominio (topologia della griglia, definizione delle azioni etc.) è parte dell’esercizio. Partendo dalla posizione iniziale della testina e combinando azioni di spostamento e colorazione, si chiede di trovare la sequenza di azioni dell’agente per raggiungere l’obiettivo.\n",
    "\n",
    "La posizione iniziale della testina, la struttura della griglia e la colorazione iniziale delle celle sono passati al sistema tramite un’immagine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inizializzazione delle librerie e moduli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# print some images from the dataset to test\\nimport matplotlib.pyplot as plt\\nfrom emnist import extract_training_samples\\nimages, labels = extract_training_samples('letters')\\nfor i in range(100):\\n    plt.subplot(10, 10, i+1)\\n    plt.axis('off')\\n    plt.imshow(images[i], cmap='gray')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import datetime\n",
    "from emnist import list_datasets\n",
    "from tensorflow import keras    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "list_datasets()\n",
    "\n",
    "'''\n",
    "# print some images from the dataset to test\n",
    "import matplotlib.pyplot as plt\n",
    "from emnist import extract_training_samples\n",
    "images, labels = extract_training_samples('letters')\n",
    "for i in range(100):\n",
    "    plt.subplot(10, 10, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[i], cmap='gray')\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descrizione formale del dominio e vincoli su cui l'agente può operare\n",
    "\n",
    "Descrizione formale del dominio e dei vincoli che le azioni eseguibili dall’agente (la testina) nella griglia devono rispettare (esempio vincoli: v1=”l’agente può compiere un solo passo alla volta”, v2=”l’agente si può muovere solo fra celle adiacenti”, etc.). La descrizione sarà un testo che descrive le regole da rispettare e le assunzioni desunte dall’analisi del dominio.\n",
    "\n",
    "### Descrizione del dominio e vincoli \n",
    "\n",
    "**Elementi del dominio:**\n",
    "- Celle: Presenti in una griglia rettangolare nella quale è possibile spostare una testina colorante. Ogni cella ha un colore di partenza ed è possibile ricolorarle con i tre colori disponibili (yellow, blue, green).\n",
    "- Testina colorante: È l'agente che, nella griglia fornita in input come immagine, può spostarsi tra le celle e cambiarne il colore. Una delle celle rappresenta la posizione iniziale della testina prima di muoversi e sulla quale dovrà ritornare dopo aver svolto le azioni richieste.\n",
    "\n",
    "**Relazioni:**\n",
    "- Ad ogni cella dell'immagine è associata un'etichetta rappresentante uno dei colori disponibili (Y, B, G).\n",
    "- La testina inizialmente dovrà essere posizionata sempre sulla cella con etichetta T.\n",
    "\n",
    "**Regole:**\n",
    "- La testina può spostarsi nelle sole direzioni nord, sud, est, ovest.\n",
    "- La testina può cambiare colore nella cella in cui è posizionata. Colorare le celle ha un costo che varia in base al colore (cost(B) = 1, cost(Y) = 2, cost(G) = 3).\n",
    "- Il passaggio da una cella all'altra ha sempre costo 1.\n",
    "- **GOAL**: Colorare tutte le celle dello stesso colore. La testina dovrà trovare un modo per farlo nella maniera più efficiente possibile, sia in termini di colori che di numero di passi effettuati per muoversi tra le celle.\n",
    "\n",
    "**Vincoli:**\n",
    "- v0=\"La griglia può essere rettangolare e quadrata\".\n",
    "- v1=”L’agente può compiere un solo passo alla volta”.\n",
    "- v2=”L’agente si può muovere solo fra celle adiacenti”.\n",
    "- v3=\"Nella griglia non esistono celle vuote, tutte devono essere colorate in partenza\".\n",
    "- v4=\"Le celle devono essere tutte raggiungibili dalla posizione iniziale della testina\".\n",
    "- v5=\"Il costo delle azioni di movimento è uniforme (1), mentre il costo della colorazione dipende dal colore scelto\".\n",
    "- v6=\"L'agente non può colorare la posizione di partenza (quindi bisogna trovare un modo per evitarlo)\".\n",
    "- v7=\"Dopo che l'agente ha colorato tutte le celle, deve ritornare alla posizione di partenza\".\n",
    "\n",
    "**Esempi di problemi, con possibili soluzioni e costi**:\n",
    "che sono costituite da un array bidimsionale, verranno fornite in un immagine come input al programma  \n",
    "\n",
    "I vincoli applicati sono i seguenti:\n",
    "- v1=”l’agente può compiere un solo passo alla volta”\n",
    "- v2=”l’agente si può muovere solo fra celle adiacenti”\n",
    "- v3=\"dopo che l'agente ha colorato tutte le testine, deve ritornare alla posizione di partenza\"\n",
    "- v4=\"l'agente non puo colorare la posizione di partenza (quindi bisogna trovare un modo per evitarlo)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Implementazione delle classi per la ricerca nello spazio degli stati di Smart Vacuum.\n",
    "Utilizzando le classi di AIMA-python si implementi quindi un dominio UniformColoring come classe derivata da Problem, scegliendo e definendo una rappresentazione per gli stati e ridefinendo opportuni metodi actions e result e tutti gli eventuali metodi aggiuntivi , es. goal_test, che si rendessero necessari.\n",
    "Si descriva e si implementi almeno una euristica definenendone le caratteristiche di consistenza e ammissibilità rispetto al dominio dato. L’euristica definita mantiene le stesse proprietà nel caso in cui le azioni di spostamento costassero 0 o le colorazioni avessero tutte lo stesso costo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Acquisizione e classificazione degli input, stato iniziale e goal. \n",
    "Si realizzi un programma che, passata in input un’immagine contenente la configurazione della griglia e la posizione iniziale dell’agente:\n",
    "a) interpreti l’immagine individuando la configurazione della griglia e, attraverso un metodo di classificazione, la posizione iniziale dell’agente e la colorazione iniziale delle celle. Non ci sono vincoli sul metodo/modello utilizzato per la classificazione. \n",
    "Si consiglia di utilizzare il dataseteMNIST/MNIST per la classficazione di lettere e cifre scritte a mano, visto a lezione e facilmente reperibile su Web. Assumiamo che le lettere siano solo maiuscole;\n",
    "b) traduca i dati risultanti dall’analisi delle immagini negli stati stato_iniziale e stato_goal del problema, secondo la rappresentazione definita per il punto 2;\n",
    "c) invochi il solutore (vedi punto 2), tramite una tecnica di ricerca informata e almeno una non informata, della classe UniformColoring e produca, se esiste, la soluzione del problema, ovvero la sequenza azioni da eseguire per raggiungere lo stato goal. E’ interessante mostrare come algoritmi diversi possano portare a soluzioni diverse nel caso in cui ottimizzino rispetto al costo della soluzione oppure rispetto la sua lunghezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named 'gzip/emnist-balanced-train-images-idx3-ubyte.gz' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (new_X_data, new_y_data)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Extract the training and test samples from the EMNIST dataset\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mextract_training_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m extract_test_samples(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Filter the training and test datasets\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/progetto-intro-ia/.venv/lib/python3.10/site-packages/emnist/__init__.py:209\u001b[0m, in \u001b[0;36mextract_training_samples\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_training_samples\u001b[39m(dataset):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract the training samples for a given dataset as a pair of numpy arrays, (images, labels). The dataset must be\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    one of those listed by list_datasets(), e.g. 'digits' or 'mnist'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextract_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/progetto-intro-ia/.venv/lib/python3.10/site-packages/emnist/__init__.py:199\u001b[0m, in \u001b[0;36mextract_samples\u001b[0;34m(dataset, usage)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_samples\u001b[39m(dataset, usage):\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract the samples for a given dataset and usage as a pair of numpy arrays, (images, labels). The dataset must\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    be one of those listed by list_datasets(), e.g. 'digits' or 'mnist'. Usage should be either 'train' or 'test'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     labels \u001b[38;5;241m=\u001b[39m extract_data(dataset, usage, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels):\n",
      "File \u001b[0;32m~/Documents/progetto-intro-ia/.venv/lib/python3.10/site-packages/emnist/__init__.py:186\u001b[0m, in \u001b[0;36mextract_data\u001b[0;34m(dataset, usage, component)\u001b[0m\n\u001b[1;32m    184\u001b[0m zip_internal_path \u001b[38;5;241m=\u001b[39m ZIP_PATH_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(dataset\u001b[38;5;241m=\u001b[39mdataset, usage\u001b[38;5;241m=\u001b[39musage, matrix\u001b[38;5;241m=\u001b[39mcomponent, dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(cache_path) \u001b[38;5;28;01mas\u001b[39;00m zf:\n\u001b[0;32m--> 186\u001b[0m     compressed_data \u001b[38;5;241m=\u001b[39m \u001b[43mzf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_internal_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m data \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mdecompress(compressed_data)\n\u001b[1;32m    188\u001b[0m array \u001b[38;5;241m=\u001b[39m parse_idx(data)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/zipfile.py:1477\u001b[0m, in \u001b[0;36mZipFile.read\u001b[0;34m(self, name, pwd)\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, pwd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return file bytes for name.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1477\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m   1478\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/zipfile.py:1516\u001b[0m, in \u001b[0;36mZipFile.open\u001b[0;34m(self, name, mode, pwd, force_zip64)\u001b[0m\n\u001b[1;32m   1513\u001b[0m     zinfo\u001b[38;5;241m.\u001b[39m_compresslevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompresslevel\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# Get info object for name\u001b[39;00m\n\u001b[0;32m-> 1516\u001b[0m     zinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_to_write(zinfo, force_zip64\u001b[38;5;241m=\u001b[39mforce_zip64)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/zipfile.py:1443\u001b[0m, in \u001b[0;36mZipFile.getinfo\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1441\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNameToInfo\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1444\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere is no item named \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m in the archive\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[0;31mKeyError\u001b[0m: \"There is no item named 'gzip/emnist-balanced-train-images-idx3-ubyte.gz' in the archive\""
     ]
    }
   ],
   "source": [
    "# Import the necessary functions from the emnist library\n",
    "from emnist import extract_training_samples\n",
    "from emnist import extract_test_samples\n",
    "\n",
    "# Define the labels for the EMNIST dataset\n",
    "LABELS = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "          'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "          'a', 'b', 'd', 'e', 'f', 'g', 'h', 'n', 'q', 'r', 't']\n",
    "\n",
    "# Define a function to remodulate the labels\n",
    "def remodulate(y):\n",
    "    \"\"\"\n",
    "    Remodulates the input array 'y' based on the labels.\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The input array to be remodulated.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The remodulated array.\n",
    "    \"\"\"\n",
    "    # Replace the labels 'T', 'B', 'Y', 'G' with 0, 1, 2, 3 respectively\n",
    "    # Define a dictionary to map the labels to their new values\n",
    "    label_map = {\"T\": 0, \"B\": 1, \"Y\": 2, \"G\": 3}\n",
    "\n",
    "    # Loop through the label_map to replace the labels with their new values\n",
    "    for label, value in label_map.items():\n",
    "        y = np.where(y == LABELS.index(label), value, y)\n",
    "\n",
    "    return y\n",
    "\n",
    "# Define a function to filter the dataset based on specific label conditions\n",
    "def filterDataset(X_data, y_data):\n",
    "    \"\"\"\n",
    "    Filters the dataset based on lables we need to use (T, B, G, Y).\n",
    "\n",
    "    Args:\n",
    "        X_data (ndarray): Input data array of shape (n_samples, 28, 28).\n",
    "        y_data (ndarray): Target labels array of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the filtered input data array and target labels array.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the dtype of y_data is not np.uint8.\n",
    "    \"\"\"\n",
    "    # Assert that the dtype of y_data is np.uint8\n",
    "    assert y_data.dtype == np.uint8\n",
    "    # Define the classes\n",
    "    classes = LABELS\n",
    "    # Initialize the new_data_size\n",
    "    new_data_size = 0\n",
    "    # Initialize the new_data_index\n",
    "    new_data_index = 0\n",
    "\n",
    "    # Loop through the y_data to count the number of records with labels 'B', 'Y', 'G', 'T'\n",
    "    for recordIndex in range(0, y_data.shape[0]):\n",
    "        currentLabel = classes[y_data[recordIndex]]\n",
    "        if currentLabel == 'B' or currentLabel == 'Y' or currentLabel == 'G' or currentLabel == 'T':\n",
    "            new_data_size += 1\n",
    "    \n",
    "    # Initialize the new_X_data and new_y_data with zeros\n",
    "    new_X_data = np.zeros((new_data_size, 28, 28), dtype = X_data.dtype)\n",
    "    new_y_data = np.zeros((new_data_size,), dtype = np.uint8)\n",
    "\n",
    "    # Loop through the y_data to filter the records with labels 'B', 'Y', 'G', 'T'\n",
    "    for recordIndex in range(0, y_data.shape[0]):\n",
    "        currentLabel = classes[y_data[recordIndex]]\n",
    "\n",
    "        if currentLabel != 'B' and currentLabel != 'Y' and currentLabel != 'G' and currentLabel != 'T':\n",
    "            continue\n",
    "        \n",
    "        new_X_data[new_data_index] = X_data[recordIndex]\n",
    "        new_y_data[new_data_index] = y_data[recordIndex]\n",
    "        new_data_index += 1\n",
    "    \n",
    "    # Assert that the new_data_index is equal to the shape of new_X_data\n",
    "    assert new_data_index == new_X_data.shape[0]\n",
    "    return (new_X_data, new_y_data)\n",
    "\n",
    "# Extract the training and test samples from the EMNIST dataset\n",
    "X_train, y_train = extract_training_samples('balanced')\n",
    "X_test, y_test = extract_test_samples('balanced')\n",
    "\n",
    "# Filter the training and test datasets\n",
    "(X_train, y_train_library) = filterDataset(X_train, y_train)\n",
    "(X_test, y_test) = filterDataset(X_test, y_test)\n",
    "\n",
    "# Remodulate the labels of the training and test datasets\n",
    "y_train_library = remodulate(y_train_library)\n",
    "y_test = remodulate(y_test)\n",
    "\n",
    "# Normalize the training and test datasets\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "Il seguente codice sta ridisponendo i dati di addestramento e test _da 3D_ (numero di campioni, larghezza dell'immagine, altezza dell'immagine) _a 2D_ (numero di campioni, larghezza dell'immagine * altezza dell'immagine).\n",
    "Molti modelli di reti neurali si aspettano di ricevere dati in un formato bidimensionale, quindi se i tuoi dati sono in un formato diverso (ad esempio, immagini rappresentate come array tridimensionali), potrebbe essere necessario eseguire il reshape per adattarli alla forma richiesta dal modello.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the training data\n",
    "print(X_train.shape)\n",
    "\n",
    "# Reshape the training data from 3D to 2D. The new shape is (number of samples, image width * image height)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]* X_train.shape[2])\n",
    "\n",
    "# Reshape the test data from 3D to 2D. The new shape is (number of samples, image width * image height)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]* X_test.shape[2])\n",
    "\n",
    "# Print the new shape of the training data\n",
    "print(X_train.shape)\n",
    "\n",
    "# Print the shape of the test data\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione della rete neurale per il modello "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Sequential model\n",
    "seq_lett_model = keras.Sequential()\n",
    "\n",
    "# Add an Input layer to the model. This layer specifies the shape of the input data (784 features).\n",
    "seq_lett_model.add(keras.Input(shape=(784,)))\n",
    "\n",
    "# Add a Dense layer to the model with 256 neurons and a ReLU activation function.\n",
    "# Dense layers are fully connected layers, meaning all the neurons in a layer are connected to those in the next layer.\n",
    "seq_lett_model.add(keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "# Add another Dense layer to the model with 128 neurons and a ReLU activation function.\n",
    "seq_lett_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "# Add a final Dense layer with 4 neurons (because we have 4 letters to detect) and a softmax activation function.\n",
    "# The softmax function is often used in the final layer of a neural network-based classifier.\n",
    "seq_lett_model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print a summary of the model, including the number of parameters and the shape of the output at each layer.\n",
    "seq_lett_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size. This is the number of samples that will be passed through the network at once.\n",
    "batch_size = 512\n",
    "\n",
    "# Set the number of epochs to 7. An epoch is one complete pass through the entire training dataset.\n",
    "epochs = 10\n",
    "\n",
    "# Compile the model. \n",
    "# We use the sparse_categorical_crossentropy loss function, which is suitable for multi-class classification problems.\n",
    "# The optimizer is set to 'adam', which is a popular choice due to its efficiency and good performance on a wide range of problems.\n",
    "# We also specify that we want to evaluate the model's accuracy during training.\n",
    "seq_lett_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model to the training data. \n",
    "# We also specify a validation split of 0.1, meaning that 10% of the training data will be used as validation data.\n",
    "# The model's performance is evaluated on this validation data at the end of each epoch.\n",
    "training_operation = seq_lett_model.fit(X_train, y_train_library, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# Save the trained model to a file so that it can be loaded later for making predictions or continuing training.\n",
    "seq_lett_model.save('seq_lett_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(training_operation.history['accuracy'])\n",
    "plt.plot(training_operation.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(training_operation.history['loss'])\n",
    "plt.plot(training_operation.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating the performance of a trained model on test data. It prints the loss and accuracy that the model achieved on the test data. The loss is a measure of how well the model can estimate the target variables, while the accuracy is the proportion of correct predictions that the model made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test data. \n",
    "# The evaluate function returns the loss value and metrics values for the model in test mode.\n",
    "# We set verbose=0 to avoid logging the detailed output during the evaluation.\n",
    "loss, accuracy =  seq_lett_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the loss value that the model achieved on the test data.\n",
    "# The loss value represents how well the model can estimate the target variables. Lower values are better.\n",
    "print(\"Test loss:\", loss)\n",
    "\n",
    "# Print the accuracy that the model achieved on the test data.\n",
    "# The accuracy is the proportion of correct predictions that the model made. Higher values are better.\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to make predictions on the test data.\n",
    "# The predict function returns the output of the last layer in the model, which in this case is the output of the softmax layer.\n",
    "y_pred = seq_lett_model.predict(X_test)\n",
    "\n",
    "# The output of the softmax layer is a vector of probabilities for each class. \n",
    "# We use the argmax function to find the index of the maximum probability, which gives us the predicted class.\n",
    "y_pred = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# Compute the confusion matrix to evaluate the accuracy of the classification.\n",
    "# The confusion matrix is a table that is often used to describe the performance of a classification model.\n",
    "# Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class.\n",
    "# The 'normalize' parameter is set to 'true', which means the confusion matrix will be normalized by row (i.e., by the number of samples in each class).\n",
    "confusionMatrix = confusion_matrix(y_test, y_pred, normalize = 'true')\n",
    "\n",
    "# Create a ConfusionMatrixDisplay object from the confusion matrix.\n",
    "# The display_labels parameter is set to the names of the classes.\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = confusionMatrix, display_labels = ['T','B','Y','G'])\n",
    "\n",
    "# Plot the confusion matrix.\n",
    "disp.plot()\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision, Recall e F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction from the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find  './manipulated_grids/' -name 'ROI_*' -exec rm {} \\;\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX \n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame was read correctly, ret is True\n",
    "    if ret:\n",
    "        # Display the frame\n",
    "        cv2.imshow('Input', frame)\n",
    "\n",
    "        # If the 'q' key is pressed, break from the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "# This will give a string like '2022-03-15_14-30-20' for March 15, 2022 at 2:30:20 PM\n",
    "timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Use the timestamp in the filename\n",
    "file_name = './manipulated_grids/grid_{}.png'.format(timestamp)\n",
    "cv2.imwrite(file_name, frame)\n",
    "\n",
    "# Release the webcam and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Load the image that was just saved\n",
    "image  = cv2.imread('./manipulated_grids/grid_2024-03-17_16-12-12.png')\n",
    "original = image.copy()\n",
    "ROI_number = 0\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Save the grayscale image\n",
    "cv2.imwrite('./manipulated_grids/gray.png', gray)\n",
    "\n",
    "# Apply Gaussian blur to the grayscale image\n",
    "blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "\n",
    "# Save the blurred image\n",
    "cv2.imwrite('./manipulated_grids/blur.png', blur)\n",
    "\n",
    "# Apply adaptive thresholding to the blurred image\n",
    "thresh = cv2.adaptiveThreshold(blur, 255, 1, 1, 11, 2)\n",
    "\n",
    "# Save the thresholded image\n",
    "cv2.imwrite('./manipulated_grids/tresh.png', thresh)\n",
    "\n",
    "# Find contours in the thresholded image\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Initialize variables for the contour with the largest area\n",
    "max_area = 0\n",
    "c = 0\n",
    "\n",
    "# Loop through the contours\n",
    "for i in contours:\n",
    "    # Calculate the area of the contour\n",
    "    area = cv2.contourArea(i)\n",
    "\n",
    "    # If the area is larger than 70*60\n",
    "    if area > 70*60:\n",
    "        # If the area is larger than the current max_area\n",
    "        if area > max_area:\n",
    "            # Update max_area and best_cnt\n",
    "            max_area = area\n",
    "            best_cnt = i\n",
    "\n",
    "        # Draw the contour on the image\n",
    "        image = cv2.drawContours(image, contours, c, (0, 255, 0), 3)\n",
    "        plt.imshow(image)\n",
    "    c+=1\n",
    "\n",
    "# Create a binary mask where the largest contour is filled with white\n",
    "mask = np.zeros((gray.shape),np.uint8)\n",
    "cv2.drawContours(mask,[best_cnt],0,255,-1)\n",
    "cv2.drawContours(mask,[best_cnt],0,0,2)\n",
    "\n",
    "# Apply the mask to the grayscale image\n",
    "out = np.zeros_like(gray)\n",
    "out[mask == 255] = gray[mask == 255]\n",
    "\n",
    "# Initialize variables\n",
    "c = 0\n",
    "n = 0\n",
    "yt = 0\n",
    "l = []\n",
    "\n",
    "# Loop through the contours again\n",
    "for i in contours:\n",
    "    # Calculate the area of the contour\n",
    "    area = cv2.contourArea(i)\n",
    "\n",
    "    # If the area is larger than 70*60\n",
    "    if area > 70*60:\n",
    "        # Get the bounding rectangle for the contour\n",
    "        x,y,w,h = cv2.boundingRect(i)\n",
    "\n",
    "        # Draw a green rectangle around the contour on the image\n",
    "        plt.imshow(cv2.rectangle(image, (x, y), (x + w, y + h), (36,12,12), 2))\n",
    "\n",
    "        # Extract the region of interest (ROI) from the original image\n",
    "        ROI = original[y:y+h, x:x+w]\n",
    "\n",
    "        # If this is not the first ROI\n",
    "        if ROI_number != 0:\n",
    "            # If the y-coordinate is the same as the previous ROI\n",
    "            if yt == y:\n",
    "                # Remove the border from the ROI and save it as an image\n",
    "                ROI = ROI[y:y+h, x:x+w]\n",
    "                cv2.imwrite('./manipulated_grids/ROI_{}.png'.format(ROI_number), ROI)\n",
    "                n+=1\n",
    "            else:\n",
    "                # Remove the border from the ROI, update yt, and save the ROI as an image\n",
    "                ROI = ROI[y:y+h, x:x+w]\n",
    "                yt = y\n",
    "                cv2.imwrite('./manipulated_grids/ROI_{}.png'.format(ROI_number), ROI)\n",
    "                n = 1\n",
    "\n",
    "        # Increment ROI_number\n",
    "        ROI_number += 1        \n",
    "\n",
    "        # Draw the contour on the image\n",
    "        plt.imshow(cv2.drawContours(image, contours, c, (0, 255, 0), 3))\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l=[]\n",
    "for i in range(1, ROI_number):\n",
    "  im = cv2.imread(\"./manipulated_grids/ROI_\"+str(i)+\".png\")\n",
    "  im = cv2.bitwise_not(im)\n",
    "  im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "  (thresh, im) = cv2.threshold(im, 127, 255, cv2.THRESH_BINARY)\n",
    "  \n",
    "  #im = im[3:67,3:67]  #rimuovi bordo\n",
    "\n",
    "  im = cv2.resize(im, dsize=(28, 28), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "  im = np.reshape(im, -1)\n",
    "  im = im.astype(\"float32\")/255\n",
    "  im = tf.expand_dims(im, 0)\n",
    "\n",
    "  prediction = seq_lett_model.predict(im)\n",
    "  max = np.where(prediction == np.amax(prediction))\n",
    "  l.append(int(max[1]))\n",
    "\n",
    "\n",
    "if n < len(l):\n",
    "    nrow= len(l)/n\n",
    "else:\n",
    "    nrow=n/len(l)\n",
    "\n",
    "nrow = int(nrow)\n",
    "\n",
    "mat = np.array(list(reversed(l)))\n",
    "\n",
    "grid=mat.reshape(nrow, n)\n",
    "show=np.copy(grid)\n",
    "show = np.where(show==0,'T',show)\n",
    "show = np.where(show=='1','B',show)\n",
    "show = np.where(show=='2','Y',show)\n",
    "show = np.where(show=='3','G',show)\n",
    "print(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Esecuzione. \n",
    "Si implementi un simulatore dell’esecuzione del piano di azioni calcolato, anche semplicemente attraverso una sequenza di immagini."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
